{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "# from util.image_pool import ImagePool\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.InstanceNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.InstanceNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torch as torch\n",
    "import torch . nn . functional as F\n",
    "\n",
    "class Generator_Model(torch.nn.Module):\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return nn.ReLU(x)\n",
    "    def sigmoid(self,x):\n",
    "        return nn.Sigmoid(x)      \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super ( Generator_Model , self ). __init__ ()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, 7, stride=1, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv1.weight)\n",
    "        self.bn1 = torch.nn.InstanceNorm2d(64)\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(64, 128, 3, stride=2, padding=0, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv2.weight)\n",
    "        self.bn2 = torch.nn.InstanceNorm2d(128)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(128, 256, 3, stride=2, padding=0, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv3.weight)\n",
    "        self.bn3 = torch.nn.InstanceNorm2d(256)\n",
    "\n",
    "        self.resnet10 = torch.nn.Sequential(*[BasicBlock(256,256) for x in range(10)])\n",
    "        \n",
    "        self.conv3_2 = torch.nn.ConvTranspose2d(256, 128, 3, stride=2, output_padding=1, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv3_2.weight)\n",
    "        self.bn3_2 = torch.nn.InstanceNorm2d(128)\n",
    "        \n",
    "        self.conv2_1 = torch.nn.ConvTranspose2d(128, 64, 3, stride=2, output_padding=1, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv2_1.weight)\n",
    "        self.bn2_1 = torch.nn.InstanceNorm2d(64)\n",
    "        \n",
    "        self.conv1_0 = torch.nn.ConvTranspose2d(64, 1, 7, stride=1, padding=0, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv1_0.weight)\n",
    "        \n",
    "        \n",
    "    def forward(self,data):\n",
    "        x = F.relu(self.bn1(self.conv1(data)))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.resnet10(x)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
    "#         print(x.shape)\n",
    "        x = F.sigmoid(self.conv1_0(x))\n",
    "#         print(x.shape)\n",
    "#         print('end')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator_Model(torch.nn.Module):\n",
    "    \n",
    "    def lrelu(self,x):\n",
    "        return nn.LeakyReLU(x)\n",
    "    def sigmoid(self,x):\n",
    "        return nn.Sigmoid(x)      \n",
    "    \n",
    "    def __init__(self):\n",
    "        super ( Discriminator_Model , self ). __init__ ()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 64, 4, stride=2, padding=1, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv1.weight)\n",
    "\n",
    "        self.conv2 = torch.nn.Conv2d(64, 256, 4, stride=2, padding=1, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv2.weight)\n",
    "        self.bn2 = torch.nn.InstanceNorm2d(256)\n",
    "\n",
    "        self.conv3 = torch.nn.Conv2d(256, 1, 1, stride=1, padding=0, bias=True)\n",
    "        torch.nn.init.xavier_normal(self.conv3.weight)\n",
    "\n",
    "    def forward(self,data):\n",
    "#         print(data.shape)\n",
    "        x = F.leaky_relu(self.conv1(data))\n",
    "#         print(x.shape)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "#         print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.G_A_to_B = Generator_Model()\n",
    "        self.G_B_to_A = Generator_Model()\n",
    "        self.D_A = Discriminator_Model()\n",
    "        self.D_B = Discriminator_Model()\n",
    "        self.D_A_m = Discriminator_Model()\n",
    "        self.D_B_m = Discriminator_Model()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.G_A_to_B = self.G_A_to_B.cuda()\n",
    "            self.G_B_to_A = self.G_B_to_A.cuda()\n",
    "            self.D_A = self.D_A.cuda()\n",
    "            self.D_B = self.D_B.cuda()\n",
    "            self.D_A_m = self.D_A_m.cuda()\n",
    "            self.D_B_m = self.D_B_m.cuda()\n",
    "\n",
    "        \n",
    "    def loss_A_to_B(self,x_A):\n",
    "        x_B_hat = self.G_A_to_B(x_A)\n",
    "        _D_B_hat = self.D_B(x_B_hat)\n",
    "        \n",
    "        return torch.norm(_D_B_hat.view(-1,16*21)-1,2,dim=1)\n",
    "        \n",
    "        \n",
    "    def loss_B_to_A(self,x_B):\n",
    "        x_A_hat = self.G_B_to_A(x_B)\n",
    "        _D_A_hat = self.D_A(x_A_hat)\n",
    "        \n",
    "        return torch.norm(_D_A_hat.view(-1,16*21)-1,2,dim=1)\n",
    "    \n",
    "    def loss_C(self,x_A,x_B):\n",
    "        x_B_hat = self.G_A_to_B(x_A)\n",
    "        x_A_hat = self.G_B_to_A(x_B)\n",
    "        x_B_til = self.G_A_to_B(x_A_hat)\n",
    "        x_A_til = self.G_B_to_A(x_B_hat)        \n",
    "        \n",
    "        return torch.norm((x_A_til-x_A).view(-1,64*84),1,dim=1)+torch.norm((x_B_til-x_B).view(-1,64*84),1,dim=1)\n",
    "    \n",
    "    def loss_G(self,_lambda,x_A,x_B):\n",
    "        a = self.loss_A_to_B(x_A)+self.loss_B_to_A(x_B)+self.loss_C(x_A,x_B)*_lambda\n",
    "        return torch.sum(a)\n",
    "    \n",
    "    def loss_DA(self,x_A,x_B):\n",
    "        _D_A = self.D_A(x_A)\n",
    "        x_A_hat = self.G_B_to_A(x_B)\n",
    "        _D_A_hat = self.D_A(x_A_hat.detach())\n",
    "        return torch.sum(0.5*(torch.norm(_D_A.view(-1,16*21)-1,2,dim=1) + torch.norm(_D_A_hat.view(-1,16*21),2,dim=1)))\n",
    "    \n",
    "    def loss_DB(self,x_A,x_B):\n",
    "        _D_B = self.D_B(x_B)\n",
    "        x_B_hat = self.G_A_to_B(x_A)\n",
    "        _D_B_hat = self.D_B(x_B_hat.detach())\n",
    "        \n",
    "        return torch.sum(0.5*(torch.norm(_D_B.view(-1,16*21)-1,2,dim=1) + torch.norm(_D_B_hat.view(-1,16*21),2,dim=1)))\n",
    "    \n",
    "    def loss_DA_m(self,_gamma,x_A,x_B,x_M):\n",
    "        x_A_hat = self.G_B_to_A(x_B)\n",
    "        _D_A_m = self.D_A_m(x_M)\n",
    "        _D_A_m_hat = self.D_A_m(x_A_hat.detach())\n",
    "        return torch.sum(0.5*_gamma*(torch.norm(_D_A_m.view(-1,16*21)-1,2,dim=1) + torch.norm(_D_A_m_hat.view(-1,16*21),2,dim=1)))\n",
    "    \n",
    "    def loss_DB_m(self,_gamma,x_A,x_B,x_M):\n",
    "        x_B_hat = self.G_A_to_B(x_A)\n",
    "        _D_B_m = self.D_B_m(x_M)\n",
    "        _D_B_m_hat = self.D_B_m(x_B_hat.detach())\n",
    "        \n",
    "        return torch.sum(0.5*_gamma*(torch.norm(_D_B_m.view(-1,16*21)-1,2,dim=1) + torch.norm(_D_B_m_hat.view(-1,16*21),2,dim=1)))\n",
    "    \n",
    "#     def loss_D(self,_gamma,x_A,x_B,x_M):\n",
    "#         a = self.loss_DA(x_A,x_B) + self.loss_DB(x_A,x_B) + _gamma*(self.loss_DA_m(x_A,x_B,x_M) + self.loss_DB_m(x_A,x_B,x_M))\n",
    "#         return torch.sum(a)\n",
    "    \n",
    "#     def total_loss(self,_gamma,_lambda):\n",
    "#         loss = self.loss_D(_gamma)+self.loss_G(_lambda)\n",
    "#         return torch.sum(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_test = np.load('../classic_test_piano.npy').astype(np.float32)\n",
    "classic_train = np.load('../classic_train_piano.npy').astype(np.float32)\n",
    "jazz_test = np.load('../jazz_test_piano.npy').astype(np.float32)\n",
    "jazz_train = np.load('../jazz_train_piano.npy').astype(np.float32)\n",
    "pop_test = np.load('../pop_test_piano.npy').astype(np.float32)\n",
    "pop_train = np.load('../pop_train_piano.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "# if True:\n",
    "    classic_test_tensor = torch.from_numpy(classic_test)\n",
    "    classic_train_tensor = torch.from_numpy(classic_train)\n",
    "    jazz_test_tensor = torch.from_numpy(jazz_test)\n",
    "    jazz_train_tensor = torch.from_numpy(jazz_train)\n",
    "    pop_test_tensor = torch.from_numpy(pop_test)\n",
    "    pop_train_tensor = torch.from_numpy(pop_train)\n",
    "\n",
    "else:\n",
    "    classic_test_tensor = torch.from_numpy(classic_test).cuda()\n",
    "    classic_train_tensor = torch.from_numpy(classic_train).cuda()\n",
    "    jazz_test_tensor = torch.from_numpy(jazz_test).cuda()\n",
    "    jazz_train_tensor = torch.from_numpy(jazz_train).cuda()\n",
    "    pop_test_tensor = torch.from_numpy(pop_test).cuda()\n",
    "    pop_train_tensor = torch.from_numpy(pop_train).cuda()\n",
    "    \n",
    "classic_test_var = Variable(classic_test_tensor.permute(0,3,1,2))\n",
    "classic_train_var = Variable(classic_train_tensor.permute(0,3,1,2))\n",
    "jazz_test_var = Variable(jazz_test_tensor.permute(0,3,1,2))\n",
    "jazz_train_var = Variable(jazz_train_tensor.permute(0,3,1,2))    \n",
    "pop_test_var = Variable(pop_test_tensor.permute(0,3,1,2))\n",
    "pop_train_var = Variable(pop_train_tensor.permute(0,3,1,2))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "x_A = classic_train_var\n",
    "x_B = pop_train_var\n",
    "x_M = torch.cat((x_A,x_B),0)\n",
    "n_M = np.random.permutation(x_M.shape[0])\n",
    "x_M = x_M[n_M]\n",
    "\n",
    "\n",
    "\n",
    "lr = .0002\n",
    "epochs = 30\n",
    "B = 8\n",
    "N = min(x_A.shape[0],x_B.shape[0])\n",
    "NB = (N + B - 1) / B\n",
    "cycleGAN = CycleGAN()\n",
    "\n",
    "\n",
    "optimizerG = torch . optim . Adam ( itertools.chain(cycleGAN.G_A_to_B.parameters(),cycleGAN.G_B_to_A.parameters()) , lr = lr, betas=(0.5, 0.999))\n",
    "optimizerDA = torch . optim . Adam ( cycleGAN.D_A.parameters(), lr = lr, betas=(0.5, 0.999))\n",
    "optimizerDB = torch . optim . Adam ( cycleGAN.D_B.parameters(), lr = lr, betas=(0.5, 0.999))\n",
    "optimizerDAM = torch . optim . Adam ( cycleGAN.D_A_m.parameters(), lr = lr, betas=(0.5, 0.999))\n",
    "optimizerDBM = torch . optim . Adam ( cycleGAN.D_B_m.parameters() , lr = lr, betas=(0.5, 0.999))\n",
    "\n",
    "def set_grad(cycleGAN,bool_val):\n",
    "    for param in cycleGAN.D_A.parameters():\n",
    "        param.requires_grad = bool_val\n",
    "    for param in cycleGAN.D_B.parameters():\n",
    "        param.requires_grad = bool_val\n",
    "    for param in cycleGAN.D_A_m.parameters():\n",
    "        param.requires_grad = bool_val\n",
    "    for param in cycleGAN.D_B_m.parameters():\n",
    "        param.requires_grad = bool_val\n",
    "\n",
    "\n",
    "    \n",
    "# if torch.cuda.is_available():\n",
    "#     cycleGAN = nn.DataParallel(cycleGAN)\n",
    "#     cycleGAN = cycleGAN.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Failed\n",
      "[1,   100] lossD: 24.063 lossG: 39663.988\n",
      "[1,   200] lossD: 24.087 lossG: 38739.016\n",
      "[1,   300] lossD: 23.793 lossG: 39285.316\n",
      "[1,   400] lossD: 25.212 lossG: 40756.703\n",
      "[1,   500] lossD: 22.945 lossG: 39214.516\n",
      "[1,   600] lossD: 22.233 lossG: 39392.504\n",
      "[1,   700] lossD: 23.235 lossG: 38744.723\n",
      "[1,   800] lossD: 21.671 lossG: 37098.090\n",
      "[1,   900] lossD: 22.290 lossG: 38847.816\n",
      "[1,  1000] lossD: 21.930 lossG: 38173.410\n",
      "[1,  1100] lossD: 21.130 lossG: 38885.668\n",
      "[1,  1200] lossD: 20.303 lossG: 38598.492\n",
      "[1,  1300] lossD: 20.728 lossG: 40148.398\n",
      "[1,  1400] lossD: 20.778 lossG: 39081.895\n",
      "[1,  1500] lossD: 20.454 lossG: 38372.094\n",
      "[1,  1600] lossD: 19.624 lossG: 38374.324\n",
      "[1,  1700] lossD: 19.551 lossG: 39174.289\n",
      "[1,  1800] lossD: 18.392 lossG: 38391.910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python2.7/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Generator_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python2.7/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/lib/python2.7/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type Discriminator_Model. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Failed\n",
      "[2,   100] lossD: 19.544 lossG: 39121.836\n",
      "[2,   200] lossD: 18.902 lossG: 38556.387\n",
      "[2,   300] lossD: 18.512 lossG: 40226.426\n",
      "[2,   400] lossD: 19.185 lossG: 39487.211\n",
      "[2,   500] lossD: 19.373 lossG: 37192.195\n",
      "[2,   600] lossD: 18.831 lossG: 37989.113\n",
      "[2,   700] lossD: 17.715 lossG: 37238.023\n",
      "[2,   800] lossD: 17.520 lossG: 38773.777\n",
      "[2,   900] lossD: 18.286 lossG: 37514.145\n",
      "[2,  1000] lossD: 17.514 lossG: 39434.617\n",
      "[2,  1100] lossD: 17.246 lossG: 40778.727\n",
      "[2,  1200] lossD: 16.772 lossG: 38204.852\n",
      "[2,  1300] lossD: 17.301 lossG: 39338.473\n",
      "[2,  1400] lossD: 16.411 lossG: 38257.613\n",
      "[2,  1500] lossD: 17.384 lossG: 37727.715\n",
      "[2,  1600] lossD: 16.624 lossG: 39653.836\n",
      "[2,  1700] lossD: 17.486 lossG: 38098.730\n",
      "[2,  1800] lossD: 16.140 lossG: 38959.801\n",
      "Load Failed\n",
      "[3,   100] lossD: 15.356 lossG: 40022.801\n",
      "[3,   200] lossD: 15.543 lossG: 37953.984\n",
      "[3,   300] lossD: 16.304 lossG: 39755.777\n",
      "[3,   400] lossD: 15.905 lossG: 39402.531\n",
      "[3,   500] lossD: 15.899 lossG: 37375.371\n",
      "[3,   600] lossD: 16.779 lossG: 35727.461\n",
      "[3,   700] lossD: 15.558 lossG: 34430.695\n",
      "[3,   800] lossD: 15.895 lossG: 36274.820\n",
      "[3,   900] lossD: 15.674 lossG: 34476.625\n",
      "[3,  1000] lossD: 16.133 lossG: 34881.387\n",
      "[3,  1100] lossD: 15.627 lossG: 34044.879\n",
      "[3,  1200] lossD: 15.411 lossG: 33854.402\n",
      "[3,  1300] lossD: 16.189 lossG: 33286.238\n",
      "[3,  1400] lossD: 13.992 lossG: 33719.109\n",
      "[3,  1500] lossD: 14.138 lossG: 35240.555\n",
      "[3,  1600] lossD: 15.288 lossG: 32809.926\n",
      "[3,  1700] lossD: 14.924 lossG: 33968.402\n",
      "[3,  1800] lossD: 14.677 lossG: 35303.645\n",
      "Load Failed\n",
      "[4,   100] lossD: 14.620 lossG: 34225.930\n",
      "[4,   200] lossD: 14.411 lossG: 34431.738\n",
      "[4,   300] lossD: 14.269 lossG: 32822.160\n",
      "[4,   400] lossD: 14.026 lossG: 35214.934\n",
      "[4,   500] lossD: 14.144 lossG: 32511.514\n",
      "[4,   600] lossD: 14.252 lossG: 34056.758\n",
      "[4,   700] lossD: 15.505 lossG: 32760.586\n",
      "[4,   800] lossD: 13.884 lossG: 37124.859\n",
      "[4,   900] lossD: 13.376 lossG: 35465.879\n",
      "[4,  1000] lossD: 13.297 lossG: 33646.902\n",
      "[4,  1100] lossD: 14.334 lossG: 34988.875\n",
      "[4,  1200] lossD: 13.401 lossG: 34324.078\n",
      "[4,  1300] lossD: 13.209 lossG: 33837.848\n",
      "[4,  1400] lossD: 12.375 lossG: 35212.613\n",
      "[4,  1500] lossD: 13.384 lossG: 34518.664\n",
      "[4,  1600] lossD: 13.236 lossG: 33762.801\n",
      "[4,  1700] lossD: 12.831 lossG: 33243.660\n",
      "[4,  1800] lossD: 12.909 lossG: 33792.418\n",
      "Load Failed\n",
      "[5,   100] lossD: 13.326 lossG: 34724.273\n",
      "[5,   200] lossD: 12.985 lossG: 34386.043\n",
      "[5,   300] lossD: 12.604 lossG: 33926.168\n",
      "[5,   400] lossD: 13.972 lossG: 33307.438\n",
      "[5,   500] lossD: 13.789 lossG: 32811.488\n",
      "[5,   600] lossD: 12.315 lossG: 33876.535\n",
      "[5,   700] lossD: 12.814 lossG: 35018.332\n",
      "[5,   800] lossD: 12.518 lossG: 33895.414\n",
      "[5,   900] lossD: 11.341 lossG: 34166.004\n",
      "[5,  1000] lossD: 12.274 lossG: 33705.926\n",
      "[5,  1100] lossD: 12.352 lossG: 33512.980\n",
      "[5,  1200] lossD: 13.143 lossG: 35593.711\n",
      "[5,  1300] lossD: 11.927 lossG: 33797.820\n",
      "[5,  1400] lossD: 13.328 lossG: 32791.598\n",
      "[5,  1500] lossD: 12.148 lossG: 34404.434\n",
      "[5,  1600] lossD: 11.488 lossG: 34053.852\n",
      "[5,  1700] lossD: 11.351 lossG: 33794.609\n",
      "[5,  1800] lossD: 11.784 lossG: 34816.902\n",
      "Load Failed\n",
      "[6,   100] lossD: 12.728 lossG: 33570.059\n",
      "[6,   200] lossD: 11.798 lossG: 33176.496\n",
      "[6,   300] lossD: 12.755 lossG: 33383.203\n",
      "[6,   400] lossD: 12.673 lossG: 34636.875\n",
      "[6,   500] lossD: 10.974 lossG: 35837.840\n",
      "[6,   600] lossD: 11.418 lossG: 33157.809\n",
      "[6,   700] lossD: 11.587 lossG: 33195.715\n",
      "[6,   800] lossD: 10.964 lossG: 33064.098\n",
      "[6,   900] lossD: 12.253 lossG: 33622.973\n",
      "[6,  1000] lossD: 11.625 lossG: 32534.906\n",
      "[6,  1100] lossD: 12.883 lossG: 33390.480\n",
      "[6,  1200] lossD: 12.075 lossG: 32560.236\n",
      "[6,  1300] lossD: 10.748 lossG: 32982.195\n",
      "[6,  1400] lossD: 11.368 lossG: 32755.920\n",
      "[6,  1500] lossD: 11.990 lossG: 32722.805\n",
      "[6,  1600] lossD: 11.241 lossG: 32301.055\n",
      "[6,  1700] lossD: 11.592 lossG: 33488.508\n",
      "[6,  1800] lossD: 11.306 lossG: 32876.977\n",
      "Load Failed\n",
      "[7,   100] lossD: 10.799 lossG: 33518.027\n",
      "[7,   200] lossD: 12.491 lossG: 31844.227\n",
      "[7,   300] lossD: 11.282 lossG: 33196.070\n",
      "[7,   400] lossD: 12.744 lossG: 32038.754\n",
      "[7,   500] lossD: 10.693 lossG: 32525.332\n",
      "[7,   600] lossD: 12.100 lossG: 32111.014\n",
      "[7,   700] lossD: 12.153 lossG: 32895.422\n",
      "[7,   800] lossD: 11.542 lossG: 32284.920\n",
      "[7,   900] lossD: 11.156 lossG: 33186.578\n",
      "[7,  1000] lossD: 10.640 lossG: 34115.398\n",
      "[7,  1100] lossD: 10.743 lossG: 32002.672\n",
      "[7,  1200] lossD: 11.525 lossG: 32895.723\n",
      "[7,  1300] lossD: 12.190 lossG: 31404.191\n",
      "[7,  1400] lossD: 11.818 lossG: 32591.027\n",
      "[7,  1500] lossD: 12.021 lossG: 31759.615\n",
      "[7,  1600] lossD: 10.454 lossG: 31774.521\n",
      "[7,  1700] lossD: 12.185 lossG: 32498.697\n",
      "[7,  1800] lossD: 11.776 lossG: 32255.934\n",
      "Load Failed\n",
      "[8,   100] lossD: 11.851 lossG: 31627.393\n",
      "[8,   200] lossD: 11.380 lossG: 31854.680\n",
      "[8,   300] lossD: 10.399 lossG: 30788.035\n",
      "[8,   400] lossD: 10.720 lossG: 29662.320\n",
      "[8,   500] lossD: 9.985 lossG: 29527.510\n",
      "[8,   600] lossD: 10.594 lossG: 29738.100\n",
      "[8,   700] lossD: 11.439 lossG: 30265.252\n",
      "[8,   800] lossD: 10.154 lossG: 28577.785\n",
      "[8,   900] lossD: 11.322 lossG: 29010.029\n",
      "[8,  1000] lossD: 11.162 lossG: 29905.984\n",
      "[8,  1100] lossD: 10.373 lossG: 29475.701\n",
      "[8,  1200] lossD: 10.139 lossG: 28630.844\n",
      "[8,  1300] lossD: 9.707 lossG: 28549.055\n",
      "[8,  1400] lossD: 11.496 lossG: 29477.359\n",
      "[8,  1500] lossD: 10.260 lossG: 28336.654\n",
      "[8,  1600] lossD: 10.831 lossG: 27787.137\n",
      "[8,  1700] lossD: 10.308 lossG: 28578.924\n",
      "[8,  1800] lossD: 11.540 lossG: 28165.074\n",
      "Load Failed\n",
      "[9,   100] lossD: 10.176 lossG: 28972.217\n",
      "[9,   200] lossD: 10.216 lossG: 28307.332\n",
      "[9,   300] lossD: 10.549 lossG: 27686.205\n",
      "[9,   400] lossD: 9.819 lossG: 27181.371\n",
      "[9,   500] lossD: 9.621 lossG: 29590.357\n",
      "[9,   600] lossD: 10.051 lossG: 27792.455\n",
      "[9,   700] lossD: 10.223 lossG: 26660.619\n",
      "[9,   800] lossD: 10.239 lossG: 28381.432\n",
      "[9,   900] lossD: 10.332 lossG: 27335.066\n",
      "[9,  1000] lossD: 9.888 lossG: 28798.432\n",
      "[9,  1100] lossD: 10.910 lossG: 27139.127\n",
      "[9,  1200] lossD: 10.078 lossG: 27795.896\n",
      "[9,  1300] lossD: 11.168 lossG: 27564.869\n",
      "[9,  1400] lossD: 11.094 lossG: 28557.920\n",
      "[9,  1500] lossD: 10.752 lossG: 27444.459\n",
      "[9,  1600] lossD: 10.083 lossG: 27330.545\n",
      "[9,  1700] lossD: 11.527 lossG: 27901.836\n",
      "[9,  1800] lossD: 9.694 lossG: 27798.430\n",
      "Load Failed\n",
      "[10,   100] lossD: 10.153 lossG: 27399.049\n",
      "[10,   200] lossD: 9.200 lossG: 28337.727\n",
      "[10,   300] lossD: 10.574 lossG: 26902.934\n",
      "[10,   400] lossD: 9.460 lossG: 27398.842\n",
      "[10,   500] lossD: 10.521 lossG: 26655.916\n",
      "[10,   600] lossD: 10.301 lossG: 28027.021\n",
      "[10,   700] lossD: 9.769 lossG: 27753.295\n",
      "[10,   800] lossD: 9.228 lossG: 27717.100\n",
      "[10,   900] lossD: 10.035 lossG: 26972.979\n",
      "[10,  1000] lossD: 10.729 lossG: 27085.727\n",
      "[10,  1100] lossD: 10.817 lossG: 27461.592\n",
      "[10,  1200] lossD: 31.173 lossG: 27870.697\n",
      "[10,  1300] lossD: 25.495 lossG: 27510.512\n",
      "[10,  1400] lossD: 23.746 lossG: 28825.975\n",
      "[10,  1500] lossD: 44.453 lossG: 26854.930\n",
      "[10,  1600] lossD: 37.726 lossG: 26934.699\n",
      "[10,  1700] lossD: 43.072 lossG: 26804.502\n",
      "[10,  1800] lossD: 30.741 lossG: 27797.750\n",
      "Load Failed\n",
      "[11,   100] lossD: 26.888 lossG: 26673.020\n",
      "[11,   200] lossD: 25.177 lossG: 27815.160\n",
      "[11,   300] lossD: 25.816 lossG: 26753.143\n",
      "[11,   400] lossD: 28.260 lossG: 26535.666\n",
      "[11,   500] lossD: 25.380 lossG: 27131.984\n",
      "[11,   600] lossD: 24.138 lossG: 27682.582\n",
      "[11,   700] lossD: 24.976 lossG: 28090.527\n",
      "[11,   800] lossD: 21.633 lossG: 26375.637\n",
      "[11,   900] lossD: 20.897 lossG: 27667.977\n",
      "[11,  1000] lossD: 18.940 lossG: 27528.455\n",
      "[11,  1100] lossD: 22.451 lossG: 28576.475\n",
      "[11,  1200] lossD: 27.018 lossG: 25938.039\n",
      "[11,  1300] lossD: 24.031 lossG: 27857.619\n",
      "[11,  1400] lossD: 33.537 lossG: 27149.541\n",
      "[11,  1500] lossD: 27.461 lossG: 26547.510\n",
      "[11,  1600] lossD: 28.424 lossG: 27838.592\n",
      "[11,  1700] lossD: 23.268 lossG: 27216.682\n",
      "[11,  1800] lossD: 25.239 lossG: 26362.566\n",
      "Load Failed\n",
      "[12,   100] lossD: 22.422 lossG: 27131.279\n",
      "[12,   200] lossD: 24.598 lossG: 26435.615\n",
      "[12,   300] lossD: 23.352 lossG: 26471.021\n",
      "[12,   400] lossD: 24.991 lossG: 27442.572\n",
      "[12,   500] lossD: 25.217 lossG: 28530.057\n",
      "[12,   600] lossD: 22.005 lossG: 26774.322\n",
      "[12,   700] lossD: 25.264 lossG: 26731.564\n",
      "[12,   800] lossD: 25.228 lossG: 26733.518\n",
      "[12,   900] lossD: 24.247 lossG: 27063.092\n",
      "[12,  1000] lossD: 20.143 lossG: 27327.602\n",
      "[12,  1100] lossD: 19.890 lossG: 27313.770\n",
      "[12,  1200] lossD: 18.533 lossG: 27647.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12,  1300] lossD: 21.445 lossG: 27106.865\n",
      "[12,  1400] lossD: 23.018 lossG: 27772.529\n",
      "[12,  1500] lossD: 18.964 lossG: 26829.902\n",
      "[12,  1600] lossD: 18.283 lossG: 27116.297\n",
      "[12,  1700] lossD: 20.599 lossG: 25934.609\n",
      "[12,  1800] lossD: 18.765 lossG: 27655.789\n",
      "Load Failed\n",
      "[13,   100] lossD: 17.996 lossG: 26946.086\n",
      "[13,   200] lossD: 17.777 lossG: 27264.996\n",
      "[13,   300] lossD: 17.466 lossG: 27620.336\n",
      "[13,   400] lossD: 20.156 lossG: 26780.537\n",
      "[13,   500] lossD: 20.123 lossG: 26036.809\n",
      "[13,   600] lossD: 23.125 lossG: 27163.684\n",
      "[13,   700] lossD: 20.224 lossG: 26751.889\n",
      "[13,   800] lossD: 17.656 lossG: 26723.955\n",
      "[13,   900] lossD: 19.381 lossG: 27587.877\n",
      "[13,  1000] lossD: 18.360 lossG: 26714.244\n",
      "[13,  1100] lossD: 19.464 lossG: 26244.299\n",
      "[13,  1200] lossD: 19.271 lossG: 27004.361\n",
      "[13,  1300] lossD: 17.042 lossG: 27900.850\n",
      "[13,  1400] lossD: 19.090 lossG: 26971.100\n",
      "[13,  1500] lossD: 15.528 lossG: 27690.229\n",
      "[13,  1600] lossD: 15.141 lossG: 26935.625\n",
      "[13,  1700] lossD: 16.665 lossG: 27453.754\n",
      "[13,  1800] lossD: 16.582 lossG: 26435.412\n",
      "Load Failed\n",
      "[14,   100] lossD: 16.015 lossG: 26222.324\n",
      "[14,   200] lossD: 15.261 lossG: 26681.600\n",
      "[14,   300] lossD: 16.120 lossG: 26909.535\n",
      "[14,   400] lossD: 17.509 lossG: 27010.637\n",
      "[14,   500] lossD: 36.696 lossG: 29580.752\n",
      "[14,   600] lossD: 24.920 lossG: 27327.510\n",
      "[14,   700] lossD: 18.338 lossG: 27167.760\n",
      "[14,   800] lossD: 16.267 lossG: 26354.482\n",
      "[14,   900] lossD: 16.353 lossG: 28087.859\n",
      "[14,  1000] lossD: 14.611 lossG: 26525.420\n",
      "[14,  1100] lossD: 14.195 lossG: 27619.027\n",
      "[14,  1200] lossD: 12.540 lossG: 27396.484\n",
      "[14,  1300] lossD: 12.552 lossG: 27682.729\n",
      "[14,  1400] lossD: 12.723 lossG: 27075.736\n",
      "[14,  1500] lossD: 11.518 lossG: 27194.182\n",
      "[14,  1600] lossD: 11.945 lossG: 27407.607\n",
      "[14,  1700] lossD: 10.964 lossG: 26629.139\n",
      "[14,  1800] lossD: 11.719 lossG: 26582.322\n",
      "Load Failed\n",
      "[15,   100] lossD: 10.763 lossG: 27048.143\n",
      "[15,   200] lossD: 10.855 lossG: 27670.162\n",
      "[15,   300] lossD: 11.293 lossG: 27079.291\n",
      "[15,   400] lossD: 10.471 lossG: 26121.723\n",
      "[15,   500] lossD: 10.897 lossG: 26477.678\n",
      "[15,   600] lossD: 11.083 lossG: 27063.805\n",
      "[15,   700] lossD: 10.432 lossG: 27639.660\n",
      "[15,   800] lossD: 10.101 lossG: 27907.162\n",
      "[15,   900] lossD: 10.502 lossG: 26167.223\n",
      "[15,  1000] lossD: 10.989 lossG: 26376.682\n",
      "[15,  1100] lossD: 12.499 lossG: 26911.629\n",
      "[15,  1200] lossD: 13.335 lossG: 26894.848\n",
      "[15,  1300] lossD: 13.723 lossG: 26814.004\n",
      "[15,  1400] lossD: 12.303 lossG: 27231.771\n",
      "[15,  1500] lossD: 11.783 lossG: 27130.814\n",
      "[15,  1600] lossD: 10.777 lossG: 27250.646\n",
      "[15,  1700] lossD: 11.156 lossG: 26297.299\n",
      "[15,  1800] lossD: 12.567 lossG: 26616.600\n",
      "Load Failed\n",
      "[16,   100] lossD: 13.132 lossG: 25801.240\n",
      "[16,   200] lossD: 12.868 lossG: 27719.512\n",
      "[16,   300] lossD: 18.795 lossG: 26796.781\n",
      "[16,   400] lossD: 17.022 lossG: 26427.109\n",
      "[16,   500] lossD: 16.197 lossG: 26170.877\n",
      "[16,   600] lossD: 15.867 lossG: 26799.637\n",
      "[16,   700] lossD: 16.304 lossG: 26747.537\n",
      "[16,   800] lossD: 15.483 lossG: 27182.148\n",
      "[16,   900] lossD: 14.224 lossG: 26467.227\n",
      "[16,  1000] lossD: 13.089 lossG: 28416.562\n",
      "[16,  1100] lossD: 14.182 lossG: 27993.430\n",
      "[16,  1200] lossD: 14.935 lossG: 27522.549\n",
      "[16,  1300] lossD: 16.020 lossG: 26017.736\n",
      "[16,  1400] lossD: 16.207 lossG: 27637.396\n",
      "[16,  1500] lossD: 13.921 lossG: 26316.064\n",
      "[16,  1600] lossD: 13.678 lossG: 26616.447\n",
      "[16,  1700] lossD: 14.360 lossG: 26463.705\n",
      "[16,  1800] lossD: 14.582 lossG: 27289.770\n",
      "Load Failed\n",
      "[17,   100] lossD: 15.745 lossG: 26672.182\n",
      "[17,   200] lossD: 16.432 lossG: 26314.771\n",
      "[17,   300] lossD: 15.650 lossG: 26675.410\n",
      "[17,   400] lossD: 14.252 lossG: 27121.020\n",
      "[17,   500] lossD: 15.372 lossG: 27376.809\n",
      "[17,   600] lossD: 15.077 lossG: 26928.545\n",
      "[17,   700] lossD: 14.695 lossG: 26430.711\n",
      "[17,   800] lossD: 13.257 lossG: 26074.797\n",
      "[17,   900] lossD: 13.903 lossG: 26976.734\n",
      "[17,  1000] lossD: 12.736 lossG: 27858.850\n",
      "[17,  1100] lossD: 12.977 lossG: 27006.600\n",
      "[17,  1200] lossD: 17.052 lossG: 26931.750\n",
      "[17,  1300] lossD: 14.313 lossG: 27561.861\n",
      "[17,  1400] lossD: 16.881 lossG: 27067.678\n",
      "[17,  1500] lossD: 15.813 lossG: 26556.008\n",
      "[17,  1600] lossD: 13.628 lossG: 26817.324\n",
      "[17,  1700] lossD: 13.761 lossG: 26566.486\n",
      "[17,  1800] lossD: 13.993 lossG: 27183.736\n",
      "Load Failed\n",
      "[18,   100] lossD: 12.887 lossG: 26511.781\n",
      "[18,   200] lossD: 13.228 lossG: 26824.719\n",
      "[18,   300] lossD: 13.445 lossG: 27156.336\n",
      "[18,   400] lossD: 12.851 lossG: 27392.572\n",
      "[18,   500] lossD: 15.686 lossG: 27122.445\n",
      "[18,   600] lossD: 15.232 lossG: 27305.012\n",
      "[18,   700] lossD: 14.433 lossG: 26008.252\n",
      "[18,   800] lossD: 13.403 lossG: 26378.047\n",
      "[18,   900] lossD: 13.317 lossG: 27195.314\n",
      "[18,  1000] lossD: 13.286 lossG: 25660.859\n",
      "[18,  1100] lossD: 13.973 lossG: 27255.990\n",
      "[18,  1200] lossD: 12.567 lossG: 26746.701\n",
      "[18,  1300] lossD: 12.404 lossG: 27407.160\n",
      "[18,  1400] lossD: 12.451 lossG: 28367.520\n",
      "[18,  1500] lossD: 15.243 lossG: 26202.527\n",
      "[18,  1600] lossD: 13.844 lossG: 25753.018\n",
      "[18,  1700] lossD: 11.086 lossG: 27098.789\n",
      "[18,  1800] lossD: 13.708 lossG: 27177.689\n",
      "Load Failed\n",
      "[19,   100] lossD: 12.796 lossG: 25919.662\n",
      "[19,   200] lossD: 12.748 lossG: 27381.949\n",
      "[19,   300] lossD: 11.111 lossG: 27586.637\n",
      "[19,   400] lossD: 40.086 lossG: 27237.504\n",
      "[19,   500] lossD: 21.635 lossG: 27877.014\n",
      "[19,   600] lossD: 15.511 lossG: 27734.990\n",
      "[19,   700] lossD: 13.505 lossG: 26823.959\n",
      "[19,   800] lossD: 19.998 lossG: 27297.252\n",
      "[19,   900] lossD: 14.736 lossG: 26749.100\n",
      "[19,  1000] lossD: 14.378 lossG: 27237.367\n",
      "[19,  1100] lossD: 17.137 lossG: 27101.566\n",
      "[19,  1200] lossD: 15.177 lossG: 26455.941\n",
      "[19,  1300] lossD: 15.460 lossG: 27777.211\n",
      "[19,  1400] lossD: 14.596 lossG: 26782.594\n",
      "[19,  1500] lossD: 13.614 lossG: 26396.467\n",
      "[19,  1600] lossD: 12.820 lossG: 27283.232\n",
      "[19,  1700] lossD: 14.221 lossG: 26149.070\n",
      "[19,  1800] lossD: 14.612 lossG: 25724.652\n",
      "Load Failed\n",
      "[20,   100] lossD: 13.614 lossG: 26302.490\n",
      "[20,   200] lossD: 14.074 lossG: 27132.779\n",
      "[20,   300] lossD: 13.388 lossG: 26798.961\n",
      "[20,   400] lossD: 15.143 lossG: 26713.004\n",
      "[20,   500] lossD: 13.471 lossG: 26725.746\n",
      "[20,   600] lossD: 13.271 lossG: 27723.930\n",
      "[20,   700] lossD: 13.526 lossG: 25638.871\n",
      "[20,   800] lossD: 12.845 lossG: 27499.447\n",
      "[20,   900] lossD: 12.795 lossG: 27200.945\n",
      "[20,  1000] lossD: 11.455 lossG: 25599.303\n",
      "[20,  1100] lossD: 12.129 lossG: 28051.875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    try:\n",
    "        cycleGAN = load_model('cycleGAN')\n",
    "    except:\n",
    "        print('Load Failed')\n",
    "        pass\n",
    "\n",
    "    running_lossD = 0.0\n",
    "    running_lossG = 0.0\n",
    "    idxminibatches_A = np.random.permutation(NB)\n",
    "    idxminibatches_B = np.random.permutation(NB)\n",
    "    idxminibatches_M = np.random.permutation(NB)\n",
    "    \n",
    "    for k in range(NB):\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        \n",
    "        i_A = idxminibatches_A[k]\n",
    "        i_B = idxminibatches_B[k]\n",
    "        i_M = idxminibatches_M[k]\n",
    "        \n",
    "        idx_A = np.arange(B*i_A, min(B*(i_A+1), N))\n",
    "        idx_B = np.arange(B*i_B, min(B*(i_B+1), N))\n",
    "        idx_M = np.arange(B*i_M, min(B*(i_M+1), N))\n",
    "        \n",
    "        inputs_A = x_A[idx_A]\n",
    "        inputs_B = x_B[idx_B]\n",
    "        inputs_M = x_M[idx_M]\n",
    "        \n",
    "#         if torch.cuda.is_available():\n",
    "#             inputs_A.data = inputs_A.data.cuda()\n",
    "#             inputs_B.data = inputs_B.data.cuda()\n",
    "#             inputs_M.data = inputs_M.data.cuda()\n",
    "\n",
    "        set_grad(cycleGAN,False)\n",
    "        optimizerG.zero_grad()\n",
    "        lossG = cycleGAN.loss_G(10,inputs_A,inputs_B)\n",
    "        lossG.backward()\n",
    "        running_lossG += lossG[0]\n",
    "        optimizerG.step()\n",
    "        del lossG\n",
    "        \n",
    "        \n",
    "    \n",
    "        set_grad(cycleGAN,True)\n",
    "        optimizerDA.zero_grad()\n",
    "        lossDA = cycleGAN.loss_DA(inputs_A,inputs_B)\n",
    "        lossDA.backward()\n",
    "        running_lossD += lossDA[0]\n",
    "        optimizerDA.step()\n",
    "        del lossDA\n",
    "\n",
    "        optimizerDB.zero_grad()\n",
    "        lossDB = cycleGAN.loss_DB(inputs_A,inputs_B)\n",
    "        lossDB.backward()\n",
    "        running_lossD += lossDB[0]\n",
    "        optimizerDB.step()\n",
    "        del lossDB\n",
    "\n",
    "        optimizerDAM.zero_grad()\n",
    "        lossDAM = cycleGAN.loss_DA_m(1,inputs_A,inputs_B,inputs_M)\n",
    "        lossDAM.backward()\n",
    "        running_lossD += lossDAM[0]\n",
    "        optimizerDAM.step()\n",
    "        del lossDAM\n",
    "\n",
    "        optimizerDBM.zero_grad()\n",
    "        lossDBM = cycleGAN.loss_DB_m(1,inputs_A,inputs_B,inputs_M)\n",
    "        lossDBM.backward()\n",
    "        running_lossD += lossDBM[0]\n",
    "        optimizerDBM.step()\n",
    "        del lossDBM\n",
    "\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        # Print statistics\n",
    "        \n",
    "        \n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] lossD: %.3f lossG: %.3f' %\n",
    "            (epoch + 1, k + 1, running_lossD / 100, running_lossG / 100))\n",
    "            running_lossG = 0.0\n",
    "            running_lossD = 0.0\n",
    "    \n",
    "    save_model(cycleGAN)\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(cycleGAN):\n",
    "    torch.save(cycleGAN.G_A_to_B,'G_A_to_B')\n",
    "    torch.save(cycleGAN.G_B_to_A,'G_B_to_A')\n",
    "    torch.save(cycleGAN.D_A,'D_A')\n",
    "    torch.save(cycleGAN.D_B,'D_B')\n",
    "    torch.save(cycleGAN.D_A_m,'D_A_m')\n",
    "    torch.save(cycleGAN.D_B_m,'D_B_m')\n",
    "    \n",
    "def load_model(cycleGAN):\n",
    "    cycleGAN.G_A_to_B = torch.load('G_A_to_B')\n",
    "    cycleGAN.G_A_to_B = torch.load('G_B_to_A')\n",
    "    cycleGAN.D_A = torch.load('D_A')\n",
    "    cycleGAN.D_B = torch.load('D_B')\n",
    "    cycleGAN.D_B_m = torch.load('D_B_m')\n",
    "    cycleGAN.D_A_m = torch.load('D_A_m')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = inputs_A.data.cpu().numpy()\n",
    "j = cycleGAN.G_A_to_B(inputs_A).data.cpu().numpy()\n",
    "np.save('i.npy',i)\n",
    "np.save('j.npy',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pretty_midi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8042714c5c2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpretty_midi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata_ones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pretty_midi"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pretty_midi\n",
    "\n",
    "def parse_data(data):\n",
    "    data_ones = data == True\n",
    "    data_zeros = data == False\n",
    "    new_data = np.zeros_like(data, dtype=np.float32)\n",
    "    new_data[data_ones] = 1.0\n",
    "    new_data[data_zeros] = 0.0\n",
    "    return new_data\n",
    "\n",
    "def to_binary(data, threshold=0.5):\n",
    "    data_ones = data >= threshold\n",
    "    data_zeros = data < threshold\n",
    "    new_data = np.zeros_like(data, dtype=np.float32)\n",
    "    new_data[data_ones] = 1.0\n",
    "    new_data[data_zeros] = 0.0\n",
    "    return new_data\n",
    "\n",
    "def save_midi(data):\n",
    "    data_binary = to_binary(data)\n",
    "    save_midis(data_binary, \"./sample_outputs_midi\")\n",
    "\n",
    "def set_piano_roll_to_instrument(piano_roll, instrument, velocity=100, tempo=120.0, beat_resolution=16):\n",
    "    # Calculate time per pixel\n",
    "    tpp = 60.0 / tempo / float(beat_resolution)\n",
    "    threshold = 60.0 / tempo / 4\n",
    "    phrase_end_time = 60.0 / tempo * 4 * piano_roll.shape[0]\n",
    "    # Create piano_roll_search that captures note onsets and offsets\n",
    "    piano_roll = piano_roll.reshape((piano_roll.shape[0] * piano_roll.shape[1], piano_roll.shape[2]))\n",
    "    piano_roll_diff = np.concatenate((np.zeros((1, 128), dtype=int), piano_roll, np.zeros((1, 128), dtype=int)))\n",
    "    piano_roll_search = np.diff(piano_roll_diff.astype(int), axis=0)\n",
    "    # Iterate through all possible(128) pitches\n",
    "\n",
    "    for note_num in range(128):\n",
    "        # Search for notes\n",
    "        start_idx = (piano_roll_search[:, note_num] > 0).nonzero()\n",
    "        start_time = list(tpp * (start_idx[0].astype(float)))\n",
    "        # print('start_time:', start_time)\n",
    "        # print(len(start_time))\n",
    "        end_idx = (piano_roll_search[:, note_num] < 0).nonzero()\n",
    "        end_time = list(tpp * (end_idx[0].astype(float)))\n",
    "        # print('end_time:', end_time)\n",
    "        # print(len(end_time))\n",
    "        duration = [pair[1] - pair[0] for pair in zip(start_time, end_time)]\n",
    "        # print('duration each note:', duration)\n",
    "        # print(len(duration))\n",
    "\n",
    "        temp_start_time = [i for i in start_time]\n",
    "        temp_end_time = [i for i in end_time]\n",
    "\n",
    "        for i in range(len(start_time)):\n",
    "            # print(start_time)\n",
    "            if start_time[i] in temp_start_time and i != len(start_time) - 1:\n",
    "                # print('i and start_time:', i, start_time[i])\n",
    "                t = []\n",
    "                current_idx = temp_start_time.index(start_time[i])\n",
    "                for j in range(current_idx + 1, len(temp_start_time)):\n",
    "                    # print(j, temp_start_time[j])\n",
    "                    if temp_start_time[j] < start_time[i] + threshold and temp_end_time[j] <= start_time[i] + threshold:\n",
    "                        # print('popped start time:', temp_start_time[j])\n",
    "                        t.append(j)\n",
    "                        # print('popped temp_start_time:', t)\n",
    "                for _ in t:\n",
    "                    temp_start_time.pop(t[0])\n",
    "                    temp_end_time.pop(t[0])\n",
    "                # print('popped temp_start_time:', temp_start_time)\n",
    "\n",
    "        start_time = temp_start_time\n",
    "        # print('After checking, start_time:', start_time)\n",
    "        # print(len(start_time))\n",
    "        end_time = temp_end_time\n",
    "        # print('After checking, end_time:', end_time)\n",
    "        # print(len(end_time))\n",
    "        duration = [pair[1] - pair[0] for pair in zip(start_time, end_time)]\n",
    "        # print('After checking, duration each note:', duration)\n",
    "        # print(len(duration))\n",
    "\n",
    "        if len(end_time) < len(start_time):\n",
    "            d = len(start_time) - len(end_time)\n",
    "            start_time = start_time[:-d]\n",
    "        # Iterate through all the searched notes\n",
    "        for idx in range(len(start_time)):\n",
    "            if duration[idx] >= threshold:\n",
    "                # Create an Note object with corresponding note number, start time and end time\n",
    "                note = pretty_midi.Note(velocity=velocity, pitch=note_num, start=start_time[idx], end=end_time[idx])\n",
    "                # Add the note to the Instrument object\n",
    "                instrument.notes.append(note)\n",
    "            else:\n",
    "                if start_time[idx] + threshold <= phrase_end_time:\n",
    "                    # Create an Note object with corresponding note number, start time and end time\n",
    "                    note = pretty_midi.Note(velocity=velocity, pitch=note_num, start=start_time[idx],\n",
    "                                            end=start_time[idx] + threshold)\n",
    "                else:\n",
    "                    # Create an Note object with corresponding note number, start time and end time\n",
    "                    note = pretty_midi.Note(velocity=velocity, pitch=note_num, start=start_time[idx],\n",
    "                                            end=phrase_end_time)\n",
    "                # Add the note to the Instrument object\n",
    "                instrument.notes.append(note)\n",
    "    # Sort the notes by their start time\n",
    "    instrument.notes.sort(key=lambda note: note.start)\n",
    "    # print(max([i.end for i in instrument.notes]))\n",
    "    # print('tpp, threshold, phrases_end_time:', tpp, threshold, phrase_end_time)\n",
    "\n",
    "def save_midis(bars, file_path, tempo=80.0):\n",
    "    padded_bars = np.concatenate((np.zeros((bars.shape[0], bars.shape[1], 24, bars.shape[3])), bars, np.zeros((bars.shape[0], bars.shape[1], 20, bars.shape[3]))), axis=2)\n",
    "    pause = np.zeros((bars.shape[0], 64, 128, bars.shape[3]))\n",
    "    images_with_pause = padded_bars\n",
    "    images_with_pause = images_with_pause.reshape(-1, 64, padded_bars.shape[2], padded_bars.shape[3])\n",
    "    images_with_pause_list = []\n",
    "    for ch_idx in range(padded_bars.shape[3]):\n",
    "        images_with_pause_list.append(images_with_pause[:, :, :, ch_idx].reshape(images_with_pause.shape[0], images_with_pause.shape[1], images_with_pause.shape[2]))\n",
    "    write_piano_rolls_to_midi(images_with_pause_list, program_nums=[0], is_drum=[False], filename=file_path, tempo=tempo, beat_resolution=4)\n",
    "\n",
    "def write_piano_rolls_to_midi(piano_rolls, program_nums=None, is_drum=None, filename='test.mid', velocity=100, tempo=120.0, beat_resolution=24):\n",
    "    if len(piano_rolls) != len(program_nums) or len(piano_rolls) != len(is_drum):\n",
    "        print(\"Error: piano_rolls and program_nums have different sizes...\")\n",
    "        return False\n",
    "    if not program_nums:\n",
    "        program_nums = [0, 0, 0]\n",
    "    if not is_drum:\n",
    "        is_drum = [False, False, False]\n",
    "    # Create a PrettyMIDI object\n",
    "    midi = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "    # Iterate through all the input instruments\n",
    "    for idx in range(len(piano_rolls)):\n",
    "        # Create an Instrument object\n",
    "        instrument = pretty_midi.Instrument(program=program_nums[idx], is_drum=is_drum[idx])\n",
    "        # Set the piano roll to the Instrument object\n",
    "        set_piano_roll_to_instrument(piano_rolls[idx], instrument, velocity, tempo, beat_resolution)\n",
    "        # Add the instrument to the PrettyMIDI object\n",
    "        midi.instruments.append(instrument)\n",
    "    # Write out the MIDI data\n",
    "    midi.write(filename)\n",
    "\n",
    "data1 = inputs_A.data.cpu()\n",
    "\n",
    "print(data1.shape)\n",
    "# data1 = to_binary(parse_data(data1))\n",
    "# save_midi(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/d7/90f34cb0d83a6c5631cf71dfe64cc1054598c843a92b400e55675cc2ac37/pip-18.1-py2.py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 9.0.1\n",
      "    Uninstalling pip-9.0.1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/req/req_set.py\", line 778, in install\n",
      "    requirement.uninstall(auto_confirm=True)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/req/req_install.py\", line 754, in uninstall\n",
      "    paths_to_remove.remove(auto_confirm)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/req/req_uninstall.py\", line 115, in remove\n",
      "    renames(path, new_path)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/utils/__init__.py\", line 267, in renames\n",
      "    shutil.move(old, new)\n",
      "  File \"/opt/conda/lib/python2.7/shutil.py\", line 317, in move\n",
      "    os.unlink(src)\n",
      "OSError: [Errno 13] Permission denied: '/opt/conda/bin/pip'\n",
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pretty-midi\n",
      "Requirement already satisfied: six in /opt/conda/lib/python2.7/site-packages (from pretty-midi)\n",
      "Collecting mido>=1.1.16 (from pretty-midi)\n",
      "  Using cached https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.7.0 in /opt/conda/lib/python2.7/site-packages (from pretty-midi)\n",
      "Installing collected packages: mido, pretty-midi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/basecommand.py\", line 215, in main\n",
      "    status = self.run(options, args)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/commands/install.py\", line 342, in run\n",
      "    prefix=options.prefix_path,\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/req/req_set.py\", line 784, in install\n",
      "    **kwargs\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/req/req_install.py\", line 851, in install\n",
      "    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/req/req_install.py\", line 1064, in move_wheel_files\n",
      "    isolated=self.isolated,\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/wheel.py\", line 345, in move_wheel_files\n",
      "    clobber(source, lib_dir, True)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/wheel.py\", line 316, in clobber\n",
      "    ensure_dir(destdir)\n",
      "  File \"/opt/conda/lib/python2.7/site-packages/pip/utils/__init__.py\", line 83, in ensure_dir\n",
      "    os.makedirs(path)\n",
      "  File \"/opt/conda/lib/python2.7/os.py\", line 157, in makedirs\n",
      "    mkdir(name, mode)\n",
      "OSError: [Errno 13] Permission denied: '/opt/conda/lib/python2.7/site-packages/mido'\n",
      "You are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "\n",
    "\n",
    "def install(package):\n",
    "    if hasattr(pip, 'main'):\n",
    "        pip.main(['install', package])\n",
    "    else:\n",
    "        pip._internal.main(['install', package])\n",
    "\n",
    "pip.main(['install','--upgrade' ,'pip'])\n",
    "install('pretty-midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "CycleGAN instance has no attribute 'named_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e04ce9ef1238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcycleGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'G'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: CycleGAN instance has no attribute 'named_parameters'"
     ]
    }
   ],
   "source": [
    "[x[0] for x in cycleGAN.named_parameters() if 'G' not in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cycleGAN.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cycleGAN.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
